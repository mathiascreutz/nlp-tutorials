{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relevance-ranked search\n",
    "\n",
    "Let's return to the indexing of toy data, as we did in the tutorial on Boolean search. This new tutorial has also been inspired by course material by Filip Ginter in Turku.\n",
    "\n",
    "Our documents now look slightly different:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\"This is a silly silly silly example\",\n",
    "             \"A better example\",\n",
    "             \"Nothing to see here nor here nor here\",\n",
    "             \"This is a great example and a long example too\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can index them as we did before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term-document matrix:\n",
      "\n",
      "[[0 0 0 1]\n",
      " [0 1 0 0]\n",
      " [1 1 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 1 0]\n",
      " [1 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 1 0]\n",
      " [0 0 1 0]\n",
      " [0 0 1 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 1]\n",
      " [0 0 1 0]\n",
      " [0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "cv = CountVectorizer(lowercase=True, binary=True)\n",
    "binary_dense_matrix = cv.fit_transform(documents).T.todense()\n",
    "\n",
    "print(\"Term-document matrix:\\n\")\n",
    "print(binary_dense_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll remove the `binary=True` optional argument from the `CountVectorizer` constructor. The default value is `binary=False`. What change can we observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term-document matrix:\n",
      "\n",
      "[[0 0 0 1]\n",
      " [0 1 0 0]\n",
      " [1 1 0 2]\n",
      " [0 0 0 1]\n",
      " [0 0 3 0]\n",
      " [1 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 2 0]\n",
      " [0 0 1 0]\n",
      " [0 0 1 0]\n",
      " [3 0 0 0]\n",
      " [1 0 0 1]\n",
      " [0 0 1 0]\n",
      " [0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(lowercase=True)\n",
    "dense_matrix = cv.fit_transform(documents).T.todense()\n",
    "\n",
    "print(\"Term-document matrix:\\n\")\n",
    "print(dense_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's recall what term each row in the matrix corresponds to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 0 is the vector for term: and\n",
      "Row 1 is the vector for term: better\n",
      "Row 2 is the vector for term: example\n",
      "Row 3 is the vector for term: great\n",
      "Row 4 is the vector for term: here\n",
      "Row 5 is the vector for term: is\n",
      "Row 6 is the vector for term: long\n",
      "Row 7 is the vector for term: nor\n",
      "Row 8 is the vector for term: nothing\n",
      "Row 9 is the vector for term: see\n",
      "Row 10 is the vector for term: silly\n",
      "Row 11 is the vector for term: this\n",
      "Row 12 is the vector for term: to\n",
      "Row 13 is the vector for term: too\n"
     ]
    }
   ],
   "source": [
    "for (row, term) in enumerate(cv.get_feature_names_out()):\n",
    "    print(\"Row\", row, \"is the vector for term:\", term)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if we run a query on the term \"example\", we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: example\n",
      "[[1 1 0 2]]\n"
     ]
    }
   ],
   "source": [
    "t2i = cv.vocabulary_  # shorter notation: t2i = term-to-index\n",
    "print(\"Query: example\")\n",
    "print(dense_matrix[t2i[\"example\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of seeing *whether* a term occurs in a document, we now see *how many times* the term occurs in each document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example occurs 1 time(s) in document: This is a silly silly silly example\n",
      "Example occurs 1 time(s) in document: A better example\n",
      "Example occurs 0 time(s) in document: Nothing to see here nor here nor here\n",
      "Example occurs 2 time(s) in document: This is a great example and a long example too\n"
     ]
    }
   ],
   "source": [
    "hits_list = np.array(dense_matrix[t2i[\"example\"]])[0]\n",
    "\n",
    "for i, nhits in enumerate(hits_list):\n",
    "    print(\"Example occurs\", nhits, \"time(s) in document:\", documents[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the number and sizes of the documents grow, we may think that the more times a search term occurs in a document, the more relevant the document is. So, if we search for \"example\" in our toy document collection, the fourth document is most relevant (2 hits), the first and second documents come next (1 hit each) and the third document is irrelevant (0 hits).\n",
    "\n",
    "If we have multiple search terms, we might think that the more times the search terms occur in total in the document, the more relevant the document is.\n",
    "\n",
    "Note that the bit-wise logical operators `AND (&)` and `OR (|)` will not work properly anymore when our matrix contains word counts. The same applies to `NOT (1 - x)`.\n",
    "\n",
    "Let's search for the most relevant document for the query *better example*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: better example\n",
      "Hits of better:         [[0 1 0 0]]\n",
      "Hits of example:        [[1 1 0 2]]\n",
      "Hits of better example: [[1 2 0 2]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Query: better example\")\n",
    "print(\"Hits of better:        \", dense_matrix[t2i[\"better\"]])\n",
    "print(\"Hits of example:       \", dense_matrix[t2i[\"example\"]])\n",
    "print(\"Hits of better example:\", dense_matrix[t2i[\"better\"]] + dense_matrix[t2i[\"example\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just added the hits together. This means that we did not search for the phrase \"better example\", nor did we search for \"better\" AND \"example\". What we did search for was some kind of \"better\" OR \"example\", in which the sum of the number of occurrences of \"better\" and \"example\" in a document determines the relevance of the document.\n",
    "\n",
    "This means that the second document, which contains one occurrence each of \"better\" and \"example\" is as good a hit as the fourth document, which contains two occurrences of \"example\" and no occurrence of \"better\".\n",
    "\n",
    "Let's execute another query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: silly example\n",
      "Hits of silly:         [[3 0 0 0]]\n",
      "Hits of example:       [[1 1 0 2]]\n",
      "Hits of silly example: [[4 1 0 2]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Query: silly example\")\n",
    "print(\"Hits of silly:        \", dense_matrix[t2i[\"silly\"]])\n",
    "print(\"Hits of example:      \", dense_matrix[t2i[\"example\"]])\n",
    "print(\"Hits of silly example:\", dense_matrix[t2i[\"silly\"]] + dense_matrix[t2i[\"example\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and also rank (sort) the results by relevance. We leave out the document without a single hit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hits: [4 1 0 2]\n",
      "List of tuples (nhits, doc_idx) where nhits > 0: [(4, 0), (1, 1), (2, 3)]\n",
      "Ranked (nhits, doc_idx) tuples: [(4, 0), (2, 3), (1, 1)]\n",
      "\n",
      "Matched the following documents, ranked highest relevance first:\n",
      "Score of 'silly example' is 4 in document: This is a silly silly silly example\n",
      "Score of 'silly example' is 2 in document: This is a great example and a long example too\n",
      "Score of 'silly example' is 1 in document: A better example\n"
     ]
    }
   ],
   "source": [
    "# We need the np.array(...)[0] code here to convert the matrix to an ordinary list:\n",
    "hits_list = np.array(dense_matrix[t2i[\"silly\"]] + dense_matrix[t2i[\"example\"]])[0]\n",
    "print(\"Hits:\", hits_list)\n",
    "\n",
    "nhits_and_doc_ids = [ (nhits, i) for i, nhits in enumerate(hits_list) if nhits > 0 ]\n",
    "print(\"List of tuples (nhits, doc_idx) where nhits > 0:\", nhits_and_doc_ids)\n",
    "\n",
    "ranked_nhits_and_doc_ids = sorted(nhits_and_doc_ids, reverse=True)\n",
    "print(\"Ranked (nhits, doc_idx) tuples:\", ranked_nhits_and_doc_ids)\n",
    "\n",
    "print(\"\\nMatched the following documents, ranked highest relevance first:\")\n",
    "for nhits, i in ranked_nhits_and_doc_ids:\n",
    "    print(\"Score of 'silly example' is\", nhits, \"in document:\", documents[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf-idf\n",
    "\n",
    "As we may guess, pure word counts are not a good indicator of relevance. Frequently occurring words are not usually very interesting from the point of view of information content.\n",
    "\n",
    "One approach to weight terms (words) by their relevance is to use *term frequency / inverse document frequency (tf-idf)* weighting. There is another [tutorial on tf-idf](tf-idf-gutenberg.ipynb) that illustrates how this weighting works.\n",
    "\n",
    "As a matter of fact, the scikit-learn library makes it easy for us to compute the tf-idf scores of terms in a document collection. Instead of the class `CountVectorizer` we can use `TfidfVectorizer`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TfidfVectorizer can be used with many different parameter values. One option is to count ordinary term frequencies. In this setup the resulting matrix should produce the same values as the one produced by the CountVectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer:\n",
      "[[ 0.  0.  0.  1.]\n",
      " [ 0.  1.  0.  0.]\n",
      " [ 1.  1.  0.  2.]\n",
      " [ 0.  0.  0.  1.]\n",
      " [ 0.  0.  3.  0.]\n",
      " [ 1.  0.  0.  1.]\n",
      " [ 0.  0.  0.  1.]\n",
      " [ 0.  0.  2.  0.]\n",
      " [ 0.  0.  1.  0.]\n",
      " [ 0.  0.  1.  0.]\n",
      " [ 3.  0.  0.  0.]\n",
      " [ 1.  0.  0.  1.]\n",
      " [ 0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  1.]]\n",
      "\n",
      "CountVectorizer:\n",
      "[[0 0 0 1]\n",
      " [0 1 0 0]\n",
      " [1 1 0 2]\n",
      " [0 0 0 1]\n",
      " [0 0 3 0]\n",
      " [1 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 2 0]\n",
      " [0 0 1 0]\n",
      " [0 0 1 0]\n",
      " [3 0 0 0]\n",
      " [1 0 0 1]\n",
      " [0 0 1 0]\n",
      " [0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# Parameters with which TfidfVectorizer does same thing as CountVectorizer\n",
    "tfv1 = TfidfVectorizer(lowercase=True, sublinear_tf=False, use_idf=False, norm=None)\n",
    "tf_matrix1 = tfv1.fit_transform(documents).T.todense()\n",
    "\n",
    "print(\"TfidfVectorizer:\")\n",
    "print(tf_matrix1)\n",
    "\n",
    "print(\"\\nCountVectorizer:\")\n",
    "print(dense_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values are the same, except that the TfidfVectorizer produces floating-point values, whereas the CountVectorizer produces integer values.\n",
    "\n",
    "Some useful parameters for the TfidfVectorizer are `sublinear_tf`, `use_idf` and `norm`.\n",
    "\n",
    "`sublinear_tf=True` uses logarithmic word frequencies instead of linear ones. That is, if a term occurs 20 times, it is not 20 times more important than a term that occurs once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer (logarithmic term frequencies):\n",
      "[[ 0.          0.          0.          1.        ]\n",
      " [ 0.          1.          0.          0.        ]\n",
      " [ 1.          1.          0.          1.69314718]\n",
      " [ 0.          0.          0.          1.        ]\n",
      " [ 0.          0.          2.09861229  0.        ]\n",
      " [ 1.          0.          0.          1.        ]\n",
      " [ 0.          0.          0.          1.        ]\n",
      " [ 0.          0.          1.69314718  0.        ]\n",
      " [ 0.          0.          1.          0.        ]\n",
      " [ 0.          0.          1.          0.        ]\n",
      " [ 2.09861229  0.          0.          0.        ]\n",
      " [ 1.          0.          0.          1.        ]\n",
      " [ 0.          0.          1.          0.        ]\n",
      " [ 0.          0.          0.          1.        ]]\n"
     ]
    }
   ],
   "source": [
    "tfv2 = TfidfVectorizer(lowercase=True, sublinear_tf=True, use_idf=False, norm=None)\n",
    "tf_matrix2 = tfv2.fit_transform(documents).T.todense()\n",
    "\n",
    "print(\"TfidfVectorizer (logarithmic term frequencies):\")\n",
    "print(tf_matrix2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`use_idf=True` factors in the inverse document frequencies. The more documents a term occurs in, the less relevant the term is, in general:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer (logarithmic term frequencies and inverse document frequencies):\n",
      "[[ 0.          0.          0.          1.91629073]\n",
      " [ 0.          1.91629073  0.          0.        ]\n",
      " [ 1.22314355  1.22314355  0.          2.07096206]\n",
      " [ 0.          0.          0.          1.91629073]\n",
      " [ 0.          0.          4.02155128  0.        ]\n",
      " [ 1.51082562  0.          0.          1.51082562]\n",
      " [ 0.          0.          0.          1.91629073]\n",
      " [ 0.          0.          3.24456225  0.        ]\n",
      " [ 0.          0.          1.91629073  0.        ]\n",
      " [ 0.          0.          1.91629073  0.        ]\n",
      " [ 4.02155128  0.          0.          0.        ]\n",
      " [ 1.51082562  0.          0.          1.51082562]\n",
      " [ 0.          0.          1.91629073  0.        ]\n",
      " [ 0.          0.          0.          1.91629073]]\n"
     ]
    }
   ],
   "source": [
    "tfv3 = TfidfVectorizer(lowercase=True, sublinear_tf=True, use_idf=True, norm=None)\n",
    "tf_matrix3 = tfv3.fit_transform(documents).T.todense()\n",
    "\n",
    "print(\"TfidfVectorizer (logarithmic term frequencies and inverse document frequencies):\")\n",
    "print(tf_matrix3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If additionally, we use the L2 norm `norm=\"l2\"` we normalize all document vectors (columns) to have a (Euclidian) length of one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer (logarithmic term frequencies and inverse document frequencies, normalized document vectors):\n",
      "[[ 0.          0.          0.          0.39494151]\n",
      " [ 0.          0.84292635  0.          0.        ]\n",
      " [ 0.25939836  0.53802897  0.          0.42681878]\n",
      " [ 0.          0.          0.          0.39494151]\n",
      " [ 0.          0.          0.65482842  0.        ]\n",
      " [ 0.32040859  0.          0.          0.31137642]\n",
      " [ 0.          0.          0.          0.39494151]\n",
      " [ 0.          0.          0.52831145  0.        ]\n",
      " [ 0.          0.          0.31202925  0.        ]\n",
      " [ 0.          0.          0.31202925  0.        ]\n",
      " [ 0.85287113  0.          0.          0.        ]\n",
      " [ 0.32040859  0.          0.          0.31137642]\n",
      " [ 0.          0.          0.31202925  0.        ]\n",
      " [ 0.          0.          0.          0.39494151]]\n"
     ]
    }
   ],
   "source": [
    "tfv4 = TfidfVectorizer(lowercase=True, sublinear_tf=True, use_idf=True, norm=\"l2\")\n",
    "tf_matrix4 = tfv4.fit_transform(documents).T.todense()\n",
    "\n",
    "print(\"TfidfVectorizer (logarithmic term frequencies and inverse document frequencies, normalized document vectors):\")\n",
    "print(tf_matrix4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can search the index in the same way as above, even if we use tf-idf weighting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: silly example\n",
      "Hits of silly:         [[ 0.85287113  0.          0.          0.        ]]\n",
      "Hits of example:       [[ 0.25939836  0.53802897  0.          0.42681878]]\n",
      "Hits of silly example: [[ 1.11226949  0.53802897  0.          0.42681878]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Query: silly example\")\n",
    "print(\"Hits of silly:        \", tf_matrix4[t2i[\"silly\"]])\n",
    "print(\"Hits of example:      \", tf_matrix4[t2i[\"example\"]])\n",
    "print(\"Hits of silly example:\", tf_matrix4[t2i[\"silly\"]] + tf_matrix4[t2i[\"example\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and we can rank the documents using the tf-idf scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hits: [ 1.11226949  0.53802897  0.          0.42681878]\n",
      "List of tuples (hits, doc_idx) where hits > 0: [(1.1122694945914164, 0), (0.53802896910335729, 1), (0.42681878177600086, 3)]\n",
      "Ranked (hits, doc_idx) tuples: [(1.1122694945914164, 0), (0.53802896910335729, 1), (0.42681878177600086, 3)]\n",
      "\n",
      "Matched the following documents, ranked highest relevance first:\n",
      "Score of 'silly example' is 1.1123 in document: This is a silly silly silly example\n",
      "Score of 'silly example' is 0.5380 in document: A better example\n",
      "Score of 'silly example' is 0.4268 in document: This is a great example and a long example too\n"
     ]
    }
   ],
   "source": [
    "hits_list4 = np.array(tf_matrix4[t2i[\"silly\"]] + tf_matrix4[t2i[\"example\"]])[0]\n",
    "print(\"Hits:\", hits_list4)\n",
    "\n",
    "hits_and_doc_ids = [ (hits, i) for i, hits in enumerate(hits_list4) if hits > 0 ]\n",
    "print(\"List of tuples (hits, doc_idx) where hits > 0:\", hits_and_doc_ids)\n",
    "\n",
    "ranked_hits_and_doc_ids = sorted(hits_and_doc_ids, reverse=True)\n",
    "print(\"Ranked (hits, doc_idx) tuples:\", ranked_hits_and_doc_ids)\n",
    "\n",
    "print(\"\\nMatched the following documents, ranked highest relevance first:\")\n",
    "for hits, i in ranked_hits_and_doc_ids:\n",
    "    print(\"Score of 'silly example' is {:.4f} in document: {:s}\".format(hits, documents[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It makes sense that the document \"This is a silly silly silly example\" comes up on the top, but why does \"A better example\" now rank higher than \"This is a great example and a long example too\"? The former one contains only one occurrence of \"example\" whereas the latter one contains two. Can you figure out the reason?\n",
    "\n",
    "### Cosine similarity\n",
    "\n",
    "When we searched the index above, we scored the documents by summing together the tf-idf values of all the terms in the search query. A more sophisticated way is to transform the query itself into a document vector, in which we score each search term using tf-idf. We then compare the query vector to each document vector in the index. The more similar the query vector is to a document vector, the more relevant that document is for our search.\n",
    "\n",
    "Let us first create a vector of our query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          0.53802897  0.          0.          0.          0.\n",
      "   0.          0.          0.          0.84292635  0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "query_vec4 = tfv4.transform([\"silly example\"]).todense()\n",
    "print(query_vec4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is actually a matrix with one row (document-term matrix). Since we have looked at term-document matrices above, let's transpose, to understand better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.53802897]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.84292635]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(query_vec4.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that only two terms have non-zero values, and they are (not surprisingly) \"example\" and \"silly\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tf-idf weight of 'example' on row 2 is: [[ 0.53802897]]\n",
      "Tf-idf weight of 'silly' on row 10 is:  [[ 0.84292635]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Tf-idf weight of 'example' on row\", t2i[\"example\"], \"is:\", query_vec4.T[t2i[\"example\"]])\n",
    "print(\"Tf-idf weight of 'silly' on row\", t2i[\"silly\"], \"is: \", query_vec4.T[t2i[\"silly\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that you understand why the score of \"silly\" is higher than that of \"example\".\n",
    "\n",
    "To compare two vectors we use *cosine similarity*, which measures the cosine of the angle between the document vectors. If all vectors are guaranteed to be of length 1, which they are when we use the L2 norm, the cosine similarity reduces to the dot product:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The score of 'silly example' is 0.8585 in document: This is a silly silly silly example\n",
      "The score of 'silly example' is 0.2895 in document: A better example\n",
      "The score of 'silly example' is 0.0000 in document: Nothing to see here nor here nor here\n",
      "The score of 'silly example' is 0.2296 in document: This is a great example and a long example too\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 4):\n",
    "    \n",
    "    # Go through each column (document vector) in the index \n",
    "    doc_vector = tf_matrix4[:, i]\n",
    "    \n",
    "    # Compute the dot product between the query vector and the document vector\n",
    "    # (Some extra stuff here to extract the number from the matrix data structure)\n",
    "    score = np.array(np.dot(query_vec4, doc_vector))[0][0]\n",
    "    \n",
    "    print(\"The score of 'silly example' is {:.4f} in document: {:s}\".format(score, documents[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the beauty with matrix and vector algebra, we don't actually need a loop, but we can do all calculations in one single dot product:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The documents have the following cosine similarities to the query: [[ 0.85847138  0.28947517  0.          0.22964087]]\n"
     ]
    }
   ],
   "source": [
    "scores = np.dot(query_vec4, tf_matrix4)\n",
    "print(\"The documents have the following cosine similarities to the query:\", scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to rank the matching documents, we can do it like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The score of 'silly example' is 0.8585 in document: This is a silly silly silly example\n",
      "The score of 'silly example' is 0.2895 in document: A better example\n",
      "The score of 'silly example' is 0.2296 in document: This is a great example and a long example too\n"
     ]
    }
   ],
   "source": [
    "ranked_scores_and_doc_ids = \\\n",
    "    sorted([ (score, i) for i, score in enumerate(np.array(scores)[0]) if score > 0], reverse=True)\n",
    "\n",
    "for score, i in ranked_scores_and_doc_ids:\n",
    "    print(\"The score of 'silly example' is {:.4f} in document: {:s}\".format(score, documents[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling up to larger document collections with sparse matrices\n",
    "\n",
    "As we saw in the tutorial on Boolean search, any real-size data requires us to use sparse matrices. Let us go though how to use sparse matrices with tf-idf weighting.\n",
    "\n",
    "First we index the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse term-document matrix with tf-idf weights:\n",
      "  (0, 3)\t0.394941507307\n",
      "  (1, 1)\t0.84292634815\n",
      "  (2, 0)\t0.259398364206\n",
      "  (2, 1)\t0.538028969103\n",
      "  (2, 3)\t0.426818781776\n",
      "  (3, 3)\t0.394941507307\n",
      "  (4, 2)\t0.654828418798\n",
      "  (5, 0)\t0.320408585717\n",
      "  (5, 3)\t0.311376420709\n",
      "  (6, 3)\t0.394941507307\n",
      "  (7, 2)\t0.528311445151\n",
      "  (8, 2)\t0.312029250155\n",
      "  (9, 2)\t0.312029250155\n",
      "  (10, 0)\t0.852871130385\n",
      "  (11, 0)\t0.320408585717\n",
      "  (11, 3)\t0.311376420709\n",
      "  (12, 2)\t0.312029250155\n",
      "  (13, 3)\t0.394941507307\n"
     ]
    }
   ],
   "source": [
    "tfv5 = TfidfVectorizer(lowercase=True, sublinear_tf=True, use_idf=True, norm=\"l2\")\n",
    "sparse_matrix = tfv5.fit_transform(documents).T.tocsr() # CSR: compressed sparse row format => order by terms\n",
    "\n",
    "print(\"Sparse term-document matrix with tf-idf weights:\")\n",
    "print(sparse_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we convert the query string to a sparse vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse one-row query matrix (horizontal vector):\n",
      "  (0, 2)\t0.538028969103\n",
      "  (0, 10)\t0.84292634815\n"
     ]
    }
   ],
   "source": [
    "# The query vector is a horizontal vector, so in order to sort by terms, we need to use CSC\n",
    "query_vec5 = tfv5.transform([\"silly example\"]).tocsc() # CSC: compressed sparse column format\n",
    "\n",
    "print(\"Sparse one-row query matrix (horizontal vector):\")\n",
    "print(query_vec5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we compute the cosine similarity (dot product). Since we are dealing with sparse matrices, any zero values are automatically left out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching documents and their scores:\n",
      "  (0, 0)\t0.858471381859\n",
      "  (0, 1)\t0.289475171594\n",
      "  (0, 3)\t0.229640869153\n"
     ]
    }
   ],
   "source": [
    "hits = np.dot(query_vec5, sparse_matrix)\n",
    "\n",
    "print(\"Matching documents and their scores:\")\n",
    "print(hits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access the document indexes like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The matching documents are: [0 1 3]\n"
     ]
    }
   ],
   "source": [
    "print(\"The matching documents are:\", hits.nonzero()[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access the tf-idf scores like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The scores of the documents are: [ 0.85847138  0.28947517  0.22964087]\n"
     ]
    }
   ],
   "source": [
    "print(\"The scores of the documents are:\", np.array(hits[hits.nonzero()])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can rank the documents by scores. It may be hard to see that this works, since the documents happen to be in the right order already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The score of 'silly example' is 0.8585 in document: This is a silly silly silly example\n",
      "The score of 'silly example' is 0.2895 in document: A better example\n",
      "The score of 'silly example' is 0.2296 in document: This is a great example and a long example too\n"
     ]
    }
   ],
   "source": [
    "ranked_scores_and_doc_ids = sorted(zip(np.array(hits[hits.nonzero()])[0], hits.nonzero()[1]), reverse=True)\n",
    "\n",
    "for score, i in ranked_scores_and_doc_ids:\n",
    "    print(\"The score of 'silly example' is {:.4f} in document: {:s}\".format(score, documents[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gutenberg corpus\n",
    "\n",
    "Let's finally index the Gutenberg corpus in NLTK, to get a feel for some real data.\n",
    "\n",
    "We start by loading the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 18 books in the collection: ['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "booknames = nltk.corpus.gutenberg.fileids()\n",
    "\n",
    "bookdata = list(nltk.corpus.gutenberg.raw(name) for name in booknames)\n",
    "\n",
    "print(\"There are\", len(bookdata), \"books in the collection:\", booknames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we index it using the TfidfVectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of terms in vocabulary: 42063\n"
     ]
    }
   ],
   "source": [
    "gv = TfidfVectorizer(lowercase=True, sublinear_tf=True, use_idf=True, norm=\"l2\")\n",
    "g_matrix = gv.fit_transform(bookdata).T.tocsr()\n",
    "\n",
    "print(\"Number of terms in vocabulary:\", len(gv.get_feature_names_out()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a function for searching this document collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_gutenberg(query_string):\n",
    "\n",
    "    # Vectorize query string\n",
    "    query_vec = gv.transform([ query_string ]).tocsc()\n",
    "\n",
    "    # Cosine similarity\n",
    "    hits = np.dot(query_vec, g_matrix)\n",
    "\n",
    "    # Rank hits\n",
    "    ranked_scores_and_doc_ids = \\\n",
    "        sorted(zip(np.array(hits[hits.nonzero()])[0], hits.nonzero()[1]),\n",
    "               reverse=True)\n",
    "    \n",
    "    # Output result\n",
    "    print(\"Your query '{:s}' matches the following documents:\".format(query_string))\n",
    "    for i, (score, doc_idx) in enumerate(ranked_scores_and_doc_ids):\n",
    "        print(\"Doc #{:d} (score: {:.4f}): {:s}\".format(i, score, booknames[doc_idx]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and run some searches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your query 'alice' matches the following documents:\n",
      "Doc #0 (score: 0.1046): carroll-alice.txt\n",
      "Doc #1 (score: 0.0106): edgeworth-parents.txt\n",
      "Doc #2 (score: 0.0092): chesterton-thursday.txt\n",
      "\n",
      "Your query 'alice entertained harriet' matches the following documents:\n",
      "Doc #0 (score: 0.0590): carroll-alice.txt\n",
      "Doc #1 (score: 0.0505): austen-emma.txt\n",
      "Doc #2 (score: 0.0092): edgeworth-parents.txt\n",
      "Doc #3 (score: 0.0052): chesterton-thursday.txt\n",
      "Doc #4 (score: 0.0045): austen-persuasion.txt\n",
      "Doc #5 (score: 0.0043): milton-paradise.txt\n",
      "Doc #6 (score: 0.0040): austen-sense.txt\n",
      "Doc #7 (score: 0.0039): chesterton-ball.txt\n",
      "Doc #8 (score: 0.0010): bible-kjv.txt\n",
      "\n",
      "Your query 'whale hunter' matches the following documents:\n",
      "Doc #0 (score: 0.0281): melville-moby_dick.txt\n",
      "Doc #1 (score: 0.0239): bryant-stories.txt\n",
      "Doc #2 (score: 0.0135): whitman-leaves.txt\n",
      "Doc #3 (score: 0.0112): chesterton-ball.txt\n",
      "Doc #4 (score: 0.0109): edgeworth-parents.txt\n",
      "Doc #5 (score: 0.0094): shakespeare-hamlet.txt\n",
      "Doc #6 (score: 0.0081): bible-kjv.txt\n",
      "Doc #7 (score: 0.0059): shakespeare-macbeth.txt\n",
      "Doc #8 (score: 0.0057): milton-paradise.txt\n",
      "\n",
      "Your query 'oh thy lord cometh' matches the following documents:\n",
      "Doc #0 (score: 0.0332): bible-kjv.txt\n",
      "Doc #1 (score: 0.0282): blake-poems.txt\n",
      "Doc #2 (score: 0.0254): shakespeare-hamlet.txt\n",
      "Doc #3 (score: 0.0231): shakespeare-macbeth.txt\n",
      "Doc #4 (score: 0.0202): shakespeare-caesar.txt\n",
      "Doc #5 (score: 0.0187): bryant-stories.txt\n",
      "Doc #6 (score: 0.0147): melville-moby_dick.txt\n",
      "Doc #7 (score: 0.0134): milton-paradise.txt\n",
      "Doc #8 (score: 0.0134): edgeworth-parents.txt\n",
      "Doc #9 (score: 0.0120): chesterton-thursday.txt\n",
      "Doc #10 (score: 0.0118): chesterton-ball.txt\n",
      "Doc #11 (score: 0.0108): chesterton-brown.txt\n",
      "Doc #12 (score: 0.0105): austen-emma.txt\n",
      "Doc #13 (score: 0.0104): austen-sense.txt\n",
      "Doc #14 (score: 0.0100): austen-persuasion.txt\n",
      "Doc #15 (score: 0.0090): carroll-alice.txt\n",
      "Doc #16 (score: 0.0085): whitman-leaves.txt\n",
      "Doc #17 (score: 0.0067): burgess-busterbrown.txt\n",
      "\n",
      "Your query 'which book should i read' matches the following documents:\n",
      "Doc #0 (score: 0.0487): carroll-alice.txt\n",
      "Doc #1 (score: 0.0417): blake-poems.txt\n",
      "Doc #2 (score: 0.0358): bryant-stories.txt\n",
      "Doc #3 (score: 0.0353): austen-persuasion.txt\n",
      "Doc #4 (score: 0.0335): burgess-busterbrown.txt\n",
      "Doc #5 (score: 0.0327): austen-sense.txt\n",
      "Doc #6 (score: 0.0316): austen-emma.txt\n",
      "Doc #7 (score: 0.0313): chesterton-brown.txt\n",
      "Doc #8 (score: 0.0296): chesterton-thursday.txt\n",
      "Doc #9 (score: 0.0288): edgeworth-parents.txt\n",
      "Doc #10 (score: 0.0284): chesterton-ball.txt\n",
      "Doc #11 (score: 0.0281): shakespeare-caesar.txt\n",
      "Doc #12 (score: 0.0246): shakespeare-macbeth.txt\n",
      "Doc #13 (score: 0.0246): milton-paradise.txt\n",
      "Doc #14 (score: 0.0224): shakespeare-hamlet.txt\n",
      "Doc #15 (score: 0.0211): whitman-leaves.txt\n",
      "Doc #16 (score: 0.0200): melville-moby_dick.txt\n",
      "Doc #17 (score: 0.0198): bible-kjv.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "search_gutenberg(\"alice\")\n",
    "search_gutenberg(\"alice entertained harriet\")\n",
    "search_gutenberg(\"whale hunter\")\n",
    "search_gutenberg(\"oh thy lord cometh\")\n",
    "search_gutenberg(\"which book should i read\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is all for now.\n",
    "\n",
    "There are many different ways term-document scores can be computed. In some approaches the query vector is not calculated in the same way as the document vectors. For instance, the idf factor may be used for query vectors, but left out from the document vectors. If you are interested, you can compare some different approaches on your data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
